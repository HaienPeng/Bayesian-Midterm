---
title: "Midterm"
author: "Haien Peng"
format: pdf
editor: visual
---

## Model

To address multicollinearity or matrix sparsity issues, we introduce a penalty term into the general linear model. The objective function is $min L(\beta)=\frac{1}{2N}||Y-X\beta||_2^2+\lambda\sum|\beta|$, which allows the coefficients to be shrunk toward zero, thereby automatically selecting important variables.

In the Group Lasso, to express that coefficients with similar features should be either retained or discarded together, we divide the coefficients into $G$ groups based on features, where each group of coefficients is denoted by $\beta_g$ with dimension $p_g$. The objective function then becomes $min L(\beta)=\frac{1}{2N}||Y-X\beta||2^2+\lambda \sum^{G}{g=1}\sqrt{p_g}||\beta_g||_2$. The regularization term is a weighted sum of the L2 norms of the group parameters. Under this objective, the entire group of parameters is penalized together — if a group is less important, all coefficients within that group are shrunk to zero.\

In the Bayesian Group LASSO, the hierarchical model can be expressed as:

$$
\begin{aligned}
y \sim N(X\beta,\sigma^2I)\\ \beta_g|\tau_g^2,\sigma2\sim N(0,\sigma^2\tau_g^2I)\\ \tau_g^2 \sim Gamma(a,b)
\end{aligned}
$$

By imposing a Gamma prior on the group variance parameter $\tau_g^2$, the Bayesian Group LASSO achieves adaptive shrinkage of entire groups of coefficients within a Bayesian framework. This yields the same group sparsity effect as the standard Group LASSO while additionally providing estimates of parameter uncertainty.

## Data

The dataset used in this study comes from the MovieLens 100K Dataset, collected and organized by the GroupLens Research Project at the University of Minnesota, USA. Since the 1990s, the MovieLens project has focused on research in collaborative filtering and recommender systems, and this dataset is one of the most classic and widely used public datasets in recommendation algorithm research.

The version of the dataset used in this study contains 100,000 rating records, 943 users, and 1,682 movies. The rating scale ranges from 1 to 5, and each user has rated at least 20 movies. The data were collected from September 19, 1997, to April 22, 1998, representing real user interaction records from the MovieLens website (movielens.umn.edu).The raw data were cleaned to remove samples with too few ratings or missing user information. Movie information includes basic metadata such as title and release date, as well as 19 genre variables (e.g., Action, Comedy, Drama, etc.). User information includes basic demographic features such as age, gender, occupation, and zip code.

When constructing the Group LASSO model, I structured the independent variables based on the semantic relationships and statistical characteristics of the variables in the MovieLens 100K dataset. The specific grouping scheme is as follows: First, in the user feature section, I retained two variables — gender and occupation. Gender is a binary variable used to describe differences in overall rating levels between male and female users; occupation is a multicategorical variable, and these columns together represent users’ occupational identities. Second, in the movie feature section, the genre information (Genres) was used as the main content-level variable, so I combined all genre variables into an integrated “genre main effect group.” Finally, to examine the preference differences of different user groups across various movie types, I constructed two interaction groups: the Gender × Genre group, which captures gender-based differences in ratings for different movie genres, and the Occupation × Genre group, which captures the preference patterns of different occupational groups for different genres.

## Experiments

First, I used a standard Group LASSO regression to determine whether the setup of these five groups was reasonable. I used the official dataset’s training and testing splits, with 80,000 samples in the training set and 20,000 in the test set. However, since the training set was too large and sampling was extremely slow, I reduced the training set to 2% of its original size—only 1,600 samples.

```{r}
# ==========================================================
# Group LASSO on MovieLens 100K (five groups, frequentist)
# ==========================================================

# ---- 0) Packages ----
# install.packages("gglasso") 
suppressPackageStartupMessages({
  library(gglasso)
})

# ---- 1) Paths & Reading ----
data_dir <- "ml-100k"   
f_train <- file.path(data_dir, "u1.base")
f_test  <- file.path(data_dir, "u1.test")
f_user  <- file.path(data_dir, "u.user")
f_item  <- file.path(data_dir, "u.item")
f_genre <- file.path(data_dir, "u.genre")


train_ratings <- read.table(f_train, sep = "\t", header = FALSE,
                            col.names = c("user_id","item_id","rating","timestamp"))
test_ratings  <- read.table(f_test, sep = "\t", header = FALSE,
                            col.names = c("user_id","item_id","rating","timestamp"))

# u.user: user id | age | gender | occupation | zip (tab)
users <- read.table(f_user, sep = "|", header = FALSE, quote = "", col.names = c("user_id","age","gender","occupation","zip"))

# u.genre: name|index
genres_tbl <- read.table(f_genre, sep = "|", header = FALSE, quote = "", fill = TRUE,
                         col.names = c("genre","idx"))
genres_tbl <- genres_tbl[!is.na(genres_tbl$idx), ]
genres <- genres_tbl[order(genres_tbl$idx), "genre"]
stopifnot(length(genres) == 19)

# u.item: movie id | title | release date | video release date | IMDb URL | 19 genre flags
item_colnames <- c("movie_id","title","release_date","video_release_date","imdb_url", paste0("genre_", genres))
items <- read.table(f_item, sep = "|", header = FALSE, quote = "", fill = TRUE, stringsAsFactors = FALSE)
items <- items[, 1:(5 + length(genres)), drop = FALSE]
colnames(items) <- item_colnames

# ---- 2) Merge to one row per rating (user × item) ----
# rename for join
colnames(items)[1] <- "item_id"
# bulid training data df_train
df_train <- merge(train_ratings, users, by = "user_id")
df_train <- merge(df_train, items, by = "item_id")

# build testing data df_test
df_test <- merge(test_ratings, users, by = "user_id")
df_test <- merge(df_test, items, by = "item_id")


set.seed(2025)
frac <- 0.02  # remain 20% training sample
idx_sub <- sample.int(nrow(df_train), size = floor(frac * nrow(df_train)), replace = FALSE)
df_train_sub <- df_train[idx_sub, , drop = FALSE]

df <- df_train_sub



# ---- 3) Feature Engineering for the five groups ----
# (A) Gender group 

df$gender <- factor(df$gender, levels = c("F","M"))
gender_M <- as.integer(df$gender == "M")
X_gender <- matrix(gender_M, ncol = 1)
colnames(X_gender) <- "gender_M"

# (B) Occupation group 
df$occupation <- factor(df$occupation)
X_occ <- model.matrix(~ occupation - 1, data = df)  # K 列
K_occ <- ncol(X_occ)

# (C) Genre main-effect group 
genre_cols <- paste0("genre_", genres)
X_genre <- as.matrix(df[, genre_cols])
X_genre <- apply(X_genre, 2, function(z) as.numeric(as.character(z)))
storage.mode(X_genre) <- "numeric"
colnames(X_genre) <- paste0("genre_", genres) 

# (D) Gender × Genre interaction group (19 columns)
X_gxg <- sweep(X_genre, 1, gender_M, `*`)
colnames(X_gxg) <- paste0("gender_M_x_", colnames(X_genre))

# (E) Occupation × Genre interaction group (K_occ * 19 columns)
make_occxgenre <- function(genre_col_vec, occ_mat, gname) {
  out <- occ_mat * as.numeric(genre_col_vec)  
  colnames(out) <- paste0(colnames(occ_mat), "_x_", gname)
  out
}
occxgenre_list <- lapply(seq_len(ncol(X_genre)), function(j) {
  make_occxgenre(X_genre[, j], X_occ, colnames(X_genre)[j])
})
X_oxg <- do.call(cbind, occxgenre_list)  # N × (K_occ*19)

# ---- 4) Stack design matrix X & build group index gid ----
X <- cbind(X_gender, X_occ, X_genre, X_gxg, X_oxg)
y <- as.numeric(df$rating)

p_gender <- ncol(X_gender)           # 1
p_occ    <- ncol(X_occ)              # K_occ
p_genre  <- ncol(X_genre)            # 19
p_gxg    <- ncol(X_gxg)              # 19
p_oxg    <- ncol(X_oxg)              # K_occ * 19

gid <- integer(ncol(X))
ptr <- 0
gid[(ptr+1):(ptr+p_gender)] <- 1;             ptr <- ptr + p_gender  # Gender group
gid[(ptr+1):(ptr+p_occ)]    <- 2;             ptr <- ptr + p_occ     # Occupation group
gid[(ptr+1):(ptr+p_genre)]  <- 3;             ptr <- ptr + p_genre   # Genre main-effect group
gid[(ptr+1):(ptr+p_gxg)]    <- 4;             ptr <- ptr + p_gxg     # Gender×Genre group
gid[(ptr+1):(ptr+p_oxg)]    <- 5;             ptr <- ptr + p_oxg     # Occupation×Genre group
stopifnot(ptr == ncol(X))

# ---- 5) Optional: standardize X and center y (gglasso 默认会标准化，但这里手动更可控) ----
varX <- apply(X, 2, var, na.rm = TRUE)

zero_var_cols <- which(varX == 0)

if (length(zero_var_cols) > 0) {
  cat("Removing", length(zero_var_cols), "constant columns with zero variance.\n")
  X <- X[, -zero_var_cols, drop = FALSE]
  gid <- gid[-zero_var_cols]  
}

X_scaled <- scale(X, center = TRUE, scale = TRUE)
X_center_train  <- attr(X_scaled, "scaled:center")
X_scale_train   <- attr(X_scaled, "scaled:scale")
y_mean_train    <- mean(y)  
y_center <- as.numeric(scale(y, center = TRUE, scale = FALSE))

# ---- 6) Fit Group LASSO (least squares) + CV for lambda ----
set.seed(2025)
fit_gl <- gglasso(x = X_scaled, y = y_center, group = gid, loss = "ls", intercept = FALSE)

cv_gl  <- cv.gglasso(x = X_scaled, y = y_center, group = gid, loss = "ls", intercept = FALSE, nfolds = 5)
lambda_min <- cv_gl$lambda.min
lambda_1se <- cv_gl$lambda.1se


# ---- 7) Coefficients at chosen lambda & group selection summary ----
coef_min <- as.matrix(coef(cv_gl, s = "lambda.min"))[-1, , drop = FALSE]  
nonzero <- abs(coef_min[,1]) > 1e-8

grp_names <- c("Gender", "Occupation", "Genre_main", "Gender×Genre", "Occupation×Genre")
grp_sizes <- c(p_gender, p_occ, p_genre, p_gxg, p_oxg)

group_selected <- tapply(nonzero, gid, any)
group_nonzero_counts <- tapply(nonzero, gid, sum)
```

```{r}
sel_df <- data.frame(
  group_id   = 1:5,
  group_name = grp_names,
  size       = grp_sizes,
  any_nonzero = as.logical(group_selected),
  nnz_in_group = as.integer(group_nonzero_counts)
)
print(sel_df, row.names = FALSE)
```

When constructing the interaction variables between occupation and movie genres, I found that some occupation categories did not rate certain genres in the sample, resulting in corresponding interaction terms taking a constant value of 0 across the entire dataset (101 items). These zero-variance features were removed before standardization to avoid numerical issues. Therefore, in the interaction terms, the model considers a total of 101 coefficients to be zero.

```{r}

# ---- 9) CV curve----
plot(cv_gl)  
cat("log lambda.min =", log(lambda_min), "\n")
cat("log lambda.1se =", log(lambda_1se), "\n")
```

According to 5-fold cross-validation, the model achieved the minimum prediction error at $log(\lambda)=-3.6684$. Using the trained parameters on the test set, the results were $RMSE=1.1410$ and $MAE=0.9469$.

```{r}
df_t <- df_test 

# (A) Gender group
df_t$gender <- factor(df_t$gender, levels = c("F","M"))
gender_M_test <- as.integer(df_t$gender == "M")
X_gender_test <- matrix(gender_M_test, ncol = 1)
colnames(X_gender_test) <- "gender_M"

# (B) Occupation group
df_t$occupation <- factor(df_t$occupation, levels = levels(df$occupation))  
X_occ_test <- model.matrix(~ occupation - 1, data = df_t)
missing_occ_cols <- setdiff(colnames(X_occ), colnames(X_occ_test))
if (length(missing_occ_cols) > 0) {
  add_zero <- matrix(0, nrow = nrow(X_occ_test), ncol = length(missing_occ_cols))
  colnames(add_zero) <- missing_occ_cols
  X_occ_test <- cbind(X_occ_test, add_zero)
}

X_occ_test <- X_occ_test[, colnames(X_occ), drop = FALSE]

# (C) Genre main
X_genre_test <- as.matrix(df_t[, genre_cols])
X_genre_test <- apply(X_genre_test, 2, function(z) as.numeric(as.character(z)))
storage.mode(X_genre_test) <- "numeric"
colnames(X_genre_test) <- paste0("genre_", genres)

# (D) Gender × Genre
X_gxg_test <- sweep(X_genre_test, 1, gender_M_test, `*`)
colnames(X_gxg_test) <- paste0("gender_M_x_", colnames(X_genre_test))

# (E) Occupation × Genre
make_occxgenre_test <- function(genre_col_vec, occ_mat, gname) {
  out <- occ_mat * as.numeric(genre_col_vec)
  colnames(out) <- paste0(colnames(occ_mat), "_x_", gname)
  out
}
occxgenre_list_test <- lapply(seq_len(ncol(X_genre_test)), function(j) {
  make_occxgenre_test(X_genre_test[, j], X_occ_test, colnames(X_genre_test)[j])
})
X_oxg_test <- do.call(cbind, occxgenre_list_test)

X_test_full <- cbind(X_gender_test, X_occ_test, X_genre_test, X_gxg_test, X_oxg_test)

if (length(zero_var_cols) > 0) {
  X_test <- X_test_full[, -zero_var_cols, drop = FALSE]
} else {
  X_test <- X_test_full
}


stopifnot(identical(colnames(X), colnames(X_test)))
X_test_centered <- sweep(X_test, 2, X_center_train, FUN = "-")
X_test_scaled   <- sweep(X_test_centered, 2, X_scale_train,  FUN = "/")

y_test <- as.numeric(df_t$rating)


y_pred_c_min  <- as.numeric(predict(cv_gl, newx = X_test_scaled, s = "lambda.min"))
y_pred_c_1se  <- as.numeric(predict(cv_gl, newx = X_test_scaled, s = "lambda.1se"))


y_pred_min <- y_pred_c_min + y_mean_train
y_pred_1se <- y_pred_c_1se + y_mean_train

rmse <- function(a,b) sqrt(mean((a-b)^2))
mae  <- function(a,b) mean(abs(a-b))

cat(sprintf("Test RMSE (lambda.min):  %.4f\n", rmse(y_test, y_pred_min)))
cat(sprintf("Test MAE  (lambda.min):  %.4f\n", mae(y_test,  y_pred_min)))
# cat(sprintf("Test RMSE (lambda.1se):  %.4f\n", rmse(y_test, y_pred_1se)))
# cat(sprintf("Test MAE  (lambda.1se):  %.4f\n", mae(y_test,  y_pred_1se)))
```

Next, the Bayesian Group LASSO was applied for modeling and inference on the dataset. The model assumes that each observation $y_i$ follows a normal distribution, that is, $y_i \sim N(X_i^{T}\beta, \sigma^2)$. To reflect the hierarchical structure of variable grouping, all features were divided into five groups: Gender, Occupation, Genre, Gender × Genre interaction, and Occupation × Genre interaction. For each parameter $\beta$, a hierarchical prior was specified as $\beta_p|\tau_{g(p)}$, where $g(p)$ denotes the group index to which the $p$-th feature belongs. The group-level parameters $\tau_g$ were assigned exponential priors $\tau_g \sim Exponential(\lambda)$. The noise term was given a prior $\sigma \sim Student\text{-}t(3,0,2.5)$.

```{r}
# ==========================================================
# 8) Bayesian Group LASSO 
# ==========================================================
suppressPackageStartupMessages({
  library(rstan)
})

rstan_options(auto_write = TRUE)
options(mc.cores = max(1L, parallel::detectCores() - 1L))

N <- nrow(X_scaled)
P <- ncol(X_scaled)
G <- 5L
gid_vec <- as.integer(gid)  # 长度P，取值 1..5

lambda_B <- 1.0

stan_data <- list(
  N = N,
  P = P,
  G = G,
  X = X_scaled,
  y = as.numeric(y_center),
  gid = gid_vec,
  lambda = lambda_B
)
```

Because direct sampling of this model leads to a “funnel-shaped” posterior geometry due to the hierarchical structure, the sampler is prone to divergence and tree-depth saturation issues. Therefore, I adopted a a non-centered parameterization to improve numerical stability. The reparameterized form is given by $z_p \sim N(0,1)$, and defined $\beta_p = \tau_{g(p)}z_p$. Under this formulation, the generative process of the model can be written as:

$$
z_p\sim N(0,1)\\
\tau_g \sim Exponential(\lambda)\\\beta_p=\tau_{g(p)}z_{p}\\ y_i|\beta,\sigma\sim N(X_i\beta,\sigma^2)\\\sigma\sim Student-t(3,0,2.5)
$$

```{r}
# ==========================================================
# non-centered parameterization
# ==========================================================
suppressPackageStartupMessages({ library(rstan) })
rstan_options(auto_write = TRUE)
options(mc.cores = max(1L, parallel::detectCores() - 1L))

stan_code_ncp <- "
data {
  int<lower=1> N; int<lower=1> P; int<lower=1> G;
  matrix[N,P] X; vector[N] y;
  int<lower=1, upper=G> gid[P];
  real<lower=0> lambda;
}
parameters {
  vector[P] z;                   
  vector<lower=0>[G] tau;        
  real<lower=0> sigma;           
}
transformed parameters {
  vector[P] beta;
  for (p in 1:P) beta[p] = tau[ gid[p] ] * z[p];
}
model {
  // prior
  z ~ normal(0, 1);
  tau ~ exponential(lambda);
  sigma ~ student_t(3, 0, 2.5);

  // likelihood
  y ~ normal(X * beta, sigma);
}
"

stan_fit_ncp <- stan(model_code = stan_code_ncp, data = stan_data,
                     seed = 2025, chains = 4, iter = 1500, warmup = 750, init = 0,
                     control = list(adapt_delta = 0.9999, max_treedepth = 50))

print(stan_fit_ncp, pars = c("sigma","tau[1]","tau[2]","tau[3]","tau[4]","tau[5]"))

post_beta_ncp <- as.matrix(stan_fit_ncp, pars = "beta")
beta_mean_ncp <- colMeans(post_beta_ncp)
y_pred_c_ncp  <- as.numeric(X_test_scaled %*% beta_mean_ncp)
y_pred_ncp    <- y_pred_c_ncp + y_mean_train

rmse_ncp <- function(a,b) sqrt(mean((a-b)^2))
mae_ncp  <- function(a,b) mean(abs(a-b))

cat(sprintf("[Bayes-GroupLASSO-NCP] Test RMSE: %.4f\n", rmse_ncp(y_test, y_pred_ncp)))
cat(sprintf("[Bayes-GroupLASSO-NCP] Test MAE : %.4f\n", mae_ncp(y_test,  y_pred_ncp)))

# traceplot
bayesplot::mcmc_trace(stan_fit_ncp, pars=c("tau[1]","tau[2]","tau[3]","tau[4]","tau[5]"))
bayesplot::mcmc_dens_overlay(stan_fit_ncp, pars=c("tau[1]","tau[2]","tau[3]","tau[4]","tau[5]"))
# R， ESS
summary(stan_fit_ncp)$summary[grep("tau", rownames(summary(stan_fit_ncp)$summary)), c("mean","Rhat","n_eff")]

```
The results show the $\hat{R}$ values of the parameter $\tau$ across the five groups are all below 1.01, which means the variances and means of all sampling chains are almost identical, with no evident mixing issues. The effective sample size (n_eff) ranges from 630 to 1760. The trace plots show that the four chains oscillate and interweave within the parameter space without any apparent trend drift. The only somewhat distinctive case is $\tau_1$, its posterior distribution exhibits a pronounced heavy-tailed feature and a relatively large mean (around 0.27). This suggests that the coefficient variance in the first group is relatively high.

Next, I further extended the prior specification in this study. Specifically, I explored alternative prior families to evaluate how different sparsity priors affect model inference and predictive performance. First I introduced a hyperprior structure, allowing each group’s shrinkage parameter $\lambda_g$ to follow a Gamma distribution, i.e., $\lambda_g \sim Gamma(a_\lambda, b_\lambda)$. This modification enables the model to adaptively adjust the degree of shrinkage for different groups based on the data.

```{r}
# ==========================================================
# A) Group-Laplace with Hyperprior
# ==========================================================
suppressPackageStartupMessages({ library(rstan) })
rstan_options(auto_write = TRUE)
options(mc.cores = max(1L, parallel::detectCores() - 1L))

stan_code_gl_hyper <- "
data {
  int<lower=1> N; int<lower=1> P; int<lower=1> G;
  matrix[N,P] X; vector[N] y; int<lower=1,upper=G> gid[P];
  real<lower=0> a_lambda;       
  real<lower=0> b_lambda;       
}
parameters {
  vector[P] z;                   
  vector[G] log_tau;             
  vector<lower=0>[G] lambda_g;   
  real<lower=0> sigma;
}
transformed parameters {
  vector<lower=0>[G] tau;        
  vector[P] beta;
  tau = exp(log_tau);
  for (p in 1:P) {
    beta[p] = tau[ gid[p] ] * z[p];
  }
}
model {
  // superprior：lambda_g ~ Gamma(a_lambda, b_lambda)
  lambda_g ~ gamma(a_lambda, b_lambda);
  
  //log p(log_tau_g | lambda_g) = log_tau_g - lambda_g * exp(log_tau_g)
  for (g in 1:G) target += log(lambda_g[g]) - lambda_g[g] * tau[g] + log_tau[g];

  // prior
  z     ~ normal(0, 1);
  sigma ~ student_t(3, 0, 2.5);

  // likelihood
  y ~ normal(X * beta, sigma);
}
"

stan_data_gl_hyper <- list(
  N = nrow(X_scaled), P = ncol(X_scaled), G = 5L,
  X = X_scaled, y = as.numeric(y_center), gid = as.integer(gid),
  a_lambda = 5.0,    
  b_lambda = 2.0
)

fit_gl_hyper <- stan(model_code = stan_code_gl_hyper, data = stan_data_gl_hyper,
                     seed = 2025, chains = 4, iter = 2000, warmup = 1000, init = 0,
                     control = list(adapt_delta = 0.9999, max_treedepth = 45))

post_gl_hyper <- as.matrix(fit_gl_hyper, pars = "beta")
beta_mean_gl_hyper <- colMeans(post_gl_hyper)
y_pred_c_gl_hyper  <- as.numeric(X_test_scaled %*% beta_mean_gl_hyper)
y_pred_gl_hyper    <- y_pred_c_gl_hyper + y_mean_train

rmse_hyper <- function(a,b) sqrt(mean((a-b)^2))
mae_hyper  <- function(a,b) mean(abs(a-b))
cat(sprintf("[GL-Hyper] Test RMSE: %.4f\n", rmse_hyper(y_test, y_pred_gl_hyper)))
cat(sprintf("[GL-Hyper] Test MAE : %.4f\n", mae_hyper(y_test,  y_pred_gl_hyper)))

print(fit_gl_hyper, pars=c("lambda_g", "tau[1]", "tau[2]", "tau[3]", "tau[4]", "tau[5]"), probs=c(0.05,0.5,0.95))
# traceplot
bayesplot::mcmc_trace(fit_gl_hyper, pars=c("tau[1]","tau[2]","tau[3]","tau[4]","tau[5]"))
bayesplot::mcmc_dens_overlay(fit_gl_hyper, pars=c("tau[1]","tau[2]","tau[3]","tau[4]","tau[5]"))
# R， ESS
summary(fit_gl_hyper)$summary[grep("tau", rownames(summary(stan_fit_ncp)$summary)), c("mean","Rhat","n_eff")]
```
Compared with the previous version, the sampling diagnostics of the Group-Laplace with Hyperprior model show that the posterior distributions and the fluctuations of the hierarchical scale parameters remain largely similar.

I replaced the original Laplace-family prior with the Group Horseshoe prior, in which each coefficient’s local scale $\lambda_p$ and the group-level global scale $\tau_g$ both follow half-Cauchy distributions, and using the non-centered parameterization $\beta_p=\tau_{g(p)}\lambda_pz_p$. 

```{r}
# ==========================================================
# B) Group Horseshoe 
# ==========================================================
stan_code_group_hs <- "
data {
  int<lower=1> N; int<lower=1> P; int<lower=1> G;
  matrix[N,P] X; vector[N] y; int<lower=1,upper=G> gid[P];
}
parameters {
  vector[P] z;                        
  vector<lower=0>[G] tau;             // half-Cauchy (group/global)
  vector<lower=0>[P] lambda;          // half-Cauchy (local)
  real<lower=0> c2;                   // slab parameter c^2
  real<lower=0> sigma;
}
transformed parameters {
  vector[P] beta;
  vector<lower=0>[P] lambda_eff;      // regularized local scales

  // Regularized horseshoe:
  // lambda_eff[p] = sqrt( c^2 * lambda[p]^2 / ( c^2 + tau[g]^2 * lambda[p]^2 ) )
  // 这里用组别的 tau[g] 替代标准 RHS 里的全局 tau
  for (p in 1:P) {
    int g = gid[p];
    real t2 = square(tau[g]);
    real l2 = square(lambda[p]);
    lambda_eff[p] = sqrt( c2 * l2 / ( c2 + t2 * l2 ) );
    beta[p] = tau[g] * lambda_eff[p] * z[p];   // 用正则化后的 lambda_eff
  }
}
model {
  // half-Cauchy cores
  tau    ~ cauchy(0, 1);
  lambda ~ cauchy(0, 1);

  // slab prior for c^2 (regularized horseshoe):
  // c2 ~ Inv-Gamma(nu/2, nu*s^2/2) with nu=4, s=2.5  ——常用稳健设置
  {
    real slab_df    = 4.0;
    real slab_scale = 2.5;
    c2 ~ inv_gamma(0.5 * slab_df, 0.5 * slab_df * square(slab_scale));
  }

  // rest
  z     ~ normal(0, 1);
  sigma ~ student_t(3, 0, 2.5);

  y ~ normal(X * beta, sigma);
}
"

stan_data_group_hs <- list(
  N = nrow(X_scaled), P = ncol(X_scaled), G = 5L,
  X = X_scaled, y = as.numeric(y_center), gid = as.integer(gid)
)

fit_group_hs <- stan(model_code = stan_code_group_hs, data = stan_data_group_hs,
                     seed = 2025, chains = 4, iter = 2000, warmup = 1000, init = 0,
                     control = list(adapt_delta = 0.999, max_treedepth = 18))

post_group_hs <- as.matrix(fit_group_hs, pars = "beta")
beta_mean_group_hs <- colMeans(post_group_hs)
y_pred_c_group_hs  <- as.numeric(X_test_scaled %*% beta_mean_group_hs)
y_pred_group_hs    <- y_pred_c_group_hs + y_mean_train
rmse <- function(a,b) sqrt(mean((a-b)^2))
mae  <- function(a,b) mean(abs(a-b))
cat(sprintf("[Group-Horseshoe] Test RMSE: %.4f\n", rmse(y_test, y_pred_group_hs)))
cat(sprintf("[Group-Horseshoe] Test MAE : %.4f\n", mae(y_test,  y_pred_group_hs)))

print(fit_group_hs, pars=c("tau[1]","tau[2]","tau[3]","tau[4]","tau[5]","sigma"), probs=c(0.05,0.5,0.95))
# traceplot
bayesplot::mcmc_trace(fit_group_hs, pars=c("tau[1]","tau[2]","tau[3]","tau[4]","tau[5]"))
bayesplot::mcmc_dens_overlay(fit_group_hs, pars=c("tau[1]","tau[2]","tau[3]","tau[4]","tau[5]"))
# R， ESS
summary(fit_group_hs)$summary[grep("tau", rownames(summary(stan_fit_ncp)$summary)), c("mean","Rhat","n_eff")]
```
